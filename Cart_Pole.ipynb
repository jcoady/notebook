{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 23.0 eps: 0.989901 avg reward (last 100): 23.0 episode loss:  0\n",
      "episode: 100 episode reward: 12.0 eps: 0.980050830419928 avg reward (last 100): 20.653465346534652 episode loss:  68.23142\n",
      "episode: 200 episode reward: 17.0 eps: 0.9702986765411791 avg reward (last 100): 22.18811881188119 episode loss:  65.00485\n",
      "episode: 300 episode reward: 18.0 eps: 0.960643563042708 avg reward (last 100): 23.85148514851485 episode loss:  184.54599\n",
      "episode: 400 episode reward: 16.0 eps: 0.9510845243085565 avg reward (last 100): 23.84158415841584 episode loss:  294.69516\n",
      "episode: 500 episode reward: 14.0 eps: 0.9416206043312847 avg reward (last 100): 22.663366336633665 episode loss:  167.42046\n",
      "episode: 600 episode reward: 13.0 eps: 0.9322508566163586 avg reward (last 100): 21.623762376237625 episode loss:  307.902\n",
      "episode: 700 episode reward: 12.0 eps: 0.9229743440874912 avg reward (last 100): 23.425742574257427 episode loss:  261.20093\n",
      "episode: 800 episode reward: 23.0 eps: 0.913790138992923 avg reward (last 100): 21.663366336633665 episode loss:  413.52527\n",
      "episode: 900 episode reward: 13.0 eps: 0.9046973228126401 avg reward (last 100): 24.73267326732673 episode loss:  462.51587\n",
      "episode: 1000 episode reward: 18.0 eps: 0.8956949861665088 avg reward (last 100): 22.217821782178216 episode loss:  183.80896\n",
      "episode: 1100 episode reward: 54.0 eps: 0.8867822287233291 avg reward (last 100): 24.594059405940595 episode loss:  545.9867\n",
      "episode: 1200 episode reward: 31.0 eps: 0.877958159110793 avg reward (last 100): 23.455445544554454 episode loss:  583.45935\n",
      "episode: 1300 episode reward: 17.0 eps: 0.8692218948263346 avg reward (last 100): 23.504950495049506 episode loss:  216.19136\n",
      "episode: 1400 episode reward: 17.0 eps: 0.8605725621488737 avg reward (last 100): 27.346534653465348 episode loss:  267.92758\n",
      "episode: 1500 episode reward: 19.0 eps: 0.8520092960514319 avg reward (last 100): 22.356435643564357 episode loss:  157.10426\n",
      "episode: 1600 episode reward: 20.0 eps: 0.843531240114621 avg reward (last 100): 21.81188118811881 episode loss:  332.11282\n",
      "episode: 1700 episode reward: 46.0 eps: 0.8351375464409935 avg reward (last 100): 22.415841584158414 episode loss:  251.15074\n",
      "episode: 1800 episode reward: 16.0 eps: 0.8268273755702407 avg reward (last 100): 25.128712871287128 episode loss:  438.2057\n",
      "episode: 1900 episode reward: 26.0 eps: 0.8185998963952398 avg reward (last 100): 24.287128712871286 episode loss:  357.1436\n",
      "episode: 2000 episode reward: 13.0 eps: 0.8104542860789328 avg reward (last 100): 23.0 episode loss:  527.4243\n",
      "episode: 2100 episode reward: 16.0 eps: 0.802389729972035 avg reward (last 100): 21.99009900990099 episode loss:  281.99026\n",
      "episode: 2200 episode reward: 57.0 eps: 0.7944054215315619 avg reward (last 100): 22.683168316831683 episode loss:  122.23093\n",
      "episode: 2300 episode reward: 24.0 eps: 0.786500562240163 avg reward (last 100): 22.980198019801982 episode loss:  573.1126\n",
      "episode: 2400 episode reward: 17.0 eps: 0.7786743615262652 avg reward (last 100): 25.247524752475247 episode loss:  186.65837\n",
      "episode: 2500 episode reward: 26.0 eps: 0.7709260366850046 avg reward (last 100): 25.326732673267326 episode loss:  471.13834\n",
      "episode: 2600 episode reward: 28.0 eps: 0.7632548127999497 avg reward (last 100): 23.445544554455445 episode loss:  452.59875\n",
      "episode: 2700 episode reward: 27.0 eps: 0.755659922665597 avg reward (last 100): 23.742574257425744 episode loss:  399.4426\n",
      "episode: 2800 episode reward: 33.0 eps: 0.7481406067106477 avg reward (last 100): 24.485148514851485 episode loss:  405.91382\n",
      "episode: 2900 episode reward: 12.0 eps: 0.7406961129220389 avg reward (last 100): 24.04950495049505 episode loss:  156.57135\n",
      "episode: 3000 episode reward: 20.0 eps: 0.7333256967697342 avg reward (last 100): 22.693069306930692 episode loss:  295.2684\n",
      "episode: 3100 episode reward: 79.0 eps: 0.7260286211322649 avg reward (last 100): 25.85148514851485 episode loss:  285.2728\n",
      "episode: 3200 episode reward: 14.0 eps: 0.7188041562230073 avg reward (last 100): 24.26732673267327 episode loss:  156.59987\n",
      "episode: 3300 episode reward: 23.0 eps: 0.7116515795171979 avg reward (last 100): 21.15841584158416 episode loss:  370.8493\n",
      "episode: 3400 episode reward: 23.0 eps: 0.7045701756796721 avg reward (last 100): 22.821782178217823 episode loss:  190.66359\n",
      "episode: 3500 episode reward: 14.0 eps: 0.6975592364933232 avg reward (last 100): 23.257425742574256 episode loss:  742.4323\n",
      "episode: 3600 episode reward: 33.0 eps: 0.6906180607882736 avg reward (last 100): 22.396039603960396 episode loss:  276.78665\n",
      "episode: 3700 episode reward: 21.0 eps: 0.6837459543717475 avg reward (last 100): 23.485148514851485 episode loss:  596.3582\n",
      "episode: 3800 episode reward: 27.0 eps: 0.6769422299586498 avg reward (last 100): 24.95049504950495 episode loss:  363.07947\n",
      "episode: 3900 episode reward: 21.0 eps: 0.6702062071028237 avg reward (last 100): 27.88118811881188 episode loss:  372.04376\n",
      "episode: 4000 episode reward: 28.0 eps: 0.6635372121290035 avg reward (last 100): 23.306930693069308 episode loss:  245.8085\n",
      "episode: 4100 episode reward: 26.0 eps: 0.656934578065436 avg reward (last 100): 21.782178217821784 episode loss:  580.3238\n",
      "episode: 4200 episode reward: 16.0 eps: 0.6503976445771799 avg reward (last 100): 25.435643564356436 episode loss:  235.48297\n",
      "episode: 4300 episode reward: 29.0 eps: 0.6439257579000631 avg reward (last 100): 27.782178217821784 episode loss:  194.21849\n",
      "episode: 4400 episode reward: 21.0 eps: 0.6375182707752978 avg reward (last 100): 25.831683168316832 episode loss:  487.5476\n",
      "episode: 4500 episode reward: 34.0 eps: 0.6311745423847503 avg reward (last 100): 23.544554455445546 episode loss:  416.6366\n",
      "episode: 4600 episode reward: 48.0 eps: 0.6248939382868509 avg reward (last 100): 23.603960396039604 episode loss:  393.18784\n",
      "episode: 4700 episode reward: 14.0 eps: 0.6186758303531436 avg reward (last 100): 25.178217821782177 episode loss:  471.8881\n",
      "episode: 4800 episode reward: 33.0 eps: 0.6125195967054651 avg reward (last 100): 23.445544554455445 episode loss:  232.70049\n",
      "episode: 4900 episode reward: 31.0 eps: 0.6064246216537515 avg reward (last 100): 25.77227722772277 episode loss:  161.75279\n",
      "episode: 5000 episode reward: 14.0 eps: 0.6003902956344622 avg reward (last 100): 22.99009900990099 episode loss:  611.7482\n",
      "episode: 5100 episode reward: 20.0 eps: 0.5944160151496163 avg reward (last 100): 24.673267326732674 episode loss:  552.12286\n",
      "episode: 5200 episode reward: 20.0 eps: 0.5885011827064386 avg reward (last 100): 23.504950495049506 episode loss:  168.01935\n",
      "episode: 5300 episode reward: 10.0 eps: 0.5826452067575998 avg reward (last 100): 24.22772277227723 episode loss:  278.3043\n",
      "episode: 5400 episode reward: 12.0 eps: 0.5768475016420588 avg reward (last 100): 26.762376237623762 episode loss:  204.07692\n",
      "episode: 5500 episode reward: 20.0 eps: 0.5711074875264902 avg reward (last 100): 26.89108910891089 episode loss:  204.13\n",
      "episode: 5600 episode reward: 24.0 eps: 0.5654245903472923 avg reward (last 100): 22.653465346534652 episode loss:  777.42847\n",
      "episode: 5700 episode reward: 12.0 eps: 0.5597982417531768 avg reward (last 100): 24.554455445544555 episode loss:  539.8363\n",
      "episode: 5800 episode reward: 16.0 eps: 0.5542278790483262 avg reward (last 100): 24.18811881188119 episode loss:  398.83597\n",
      "episode: 5900 episode reward: 36.0 eps: 0.5487129451361167 avg reward (last 100): 24.84158415841584 episode loss:  506.41714\n",
      "episode: 6000 episode reward: 42.0 eps: 0.543252888463407 avg reward (last 100): 23.831683168316832 episode loss:  488.7512\n",
      "episode: 6100 episode reward: 25.0 eps: 0.5378471629653735 avg reward (last 100): 23.475247524752476 episode loss:  329.89026\n",
      "episode: 6200 episode reward: 19.0 eps: 0.5324952280108983 avg reward (last 100): 27.217821782178216 episode loss:  204.36078\n",
      "episode: 6300 episode reward: 27.0 eps: 0.5271965483485016 avg reward (last 100): 26.455445544554454 episode loss:  212.62523\n",
      "episode: 6400 episode reward: 28.0 eps: 0.5219505940528081 avg reward (last 100): 20.425742574257427 episode loss:  514.2287\n",
      "episode: 6500 episode reward: 31.0 eps: 0.5167568404715517 avg reward (last 100): 25.198019801980198 episode loss:  304.08102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6600 episode reward: 16.0 eps: 0.5116147681731024 avg reward (last 100): 24.504950495049506 episode loss:  185.02132\n",
      "episode: 6700 episode reward: 59.0 eps: 0.5065238628945193 avg reward (last 100): 27.386138613861387 episode loss:  277.4452\n",
      "episode: 6800 episode reward: 14.0 eps: 0.501483615490118 avg reward (last 100): 27.0 episode loss:  186.41364\n",
      "episode: 6900 episode reward: 26.0 eps: 0.4964935218805499 avg reward (last 100): 25.861386138613863 episode loss:  199.8156\n",
      "episode: 7000 episode reward: 12.0 eps: 0.49155308300238854 avg reward (last 100): 24.257425742574256 episode loss:  379.40692\n",
      "episode: 7100 episode reward: 29.0 eps: 0.48666180475821974 avg reward (last 100): 28.465346534653467 episode loss:  220.12282\n",
      "episode: 7200 episode reward: 13.0 eps: 0.48181919796722483 avg reward (last 100): 24.22772277227723 episode loss:  707.77167\n",
      "episode: 7300 episode reward: 14.0 eps: 0.477024778316258 avg reward (last 100): 25.485148514851485 episode loss:  448.20355\n",
      "episode: 7400 episode reward: 13.0 eps: 0.47227806631140934 avg reward (last 100): 24.91089108910891 episode loss:  498.32257\n",
      "episode: 7500 episode reward: 61.0 eps: 0.4675785872300506 avg reward (last 100): 26.564356435643564 episode loss:  431.63614\n",
      "episode: 7600 episode reward: 21.0 eps: 0.4629258710733578 avg reward (last 100): 25.564356435643564 episode loss:  766.5776\n",
      "episode: 7700 episode reward: 44.0 eps: 0.45831945251930506 avg reward (last 100): 27.524752475247524 episode loss:  333.81107\n",
      "episode: 7800 episode reward: 31.0 eps: 0.45375887087612954 avg reward (last 100): 28.445544554455445 episode loss:  291.49127\n",
      "episode: 7900 episode reward: 13.0 eps: 0.4492436700362556 avg reward (last 100): 25.594059405940595 episode loss:  319.07825\n",
      "episode: 8000 episode reward: 20.0 eps: 0.44477339843067976 avg reward (last 100): 22.801980198019802 episode loss:  370.12125\n",
      "episode: 8100 episode reward: 22.0 eps: 0.4403476089838086 avg reward (last 100): 28.316831683168317 episode loss:  324.0254\n",
      "episode: 8200 episode reward: 55.0 eps: 0.43596585906874646 avg reward (last 100): 25.504950495049506 episode loss:  411.24255\n",
      "episode: 8300 episode reward: 23.0 eps: 0.43162771046302806 avg reward (last 100): 27.277227722772277 episode loss:  345.75998\n",
      "episode: 8400 episode reward: 29.0 eps: 0.4273327293047919 avg reward (last 100): 25.019801980198018 episode loss:  343.96277\n",
      "episode: 8500 episode reward: 49.0 eps: 0.42308048604938847 avg reward (last 100): 31.14851485148515 episode loss:  524.7777\n",
      "episode: 8600 episode reward: 26.0 eps: 0.41887055542642126 avg reward (last 100): 30.722772277227723 episode loss:  169.07791\n",
      "episode: 8700 episode reward: 32.0 eps: 0.4147025163972157 avg reward (last 100): 28.138613861386137 episode loss:  265.78052\n",
      "episode: 8800 episode reward: 10.0 eps: 0.4105759521127113 avg reward (last 100): 27.356435643564357 episode loss:  633.9396\n",
      "episode: 8900 episode reward: 32.0 eps: 0.4064904498717701 avg reward (last 100): 32.81188118811881 episode loss:  334.58978\n",
      "episode: 9000 episode reward: 14.0 eps: 0.4024456010799042 avg reward (last 100): 29.95049504950495 episode loss:  318.50024\n",
      "episode: 9100 episode reward: 14.0 eps: 0.3984410012084108 avg reward (last 100): 30.96039603960396 episode loss:  224.2004\n",
      "episode: 9200 episode reward: 23.0 eps: 0.3944762497539148 avg reward (last 100): 31.722772277227723 episode loss:  262.00763\n",
      "episode: 9300 episode reward: 22.0 eps: 0.39055095019831576 avg reward (last 100): 28.485148514851485 episode loss:  242.1933\n",
      "episode: 9400 episode reward: 15.0 eps: 0.3866647099691295 avg reward (last 100): 29.22772277227723 episode loss:  673.80896\n",
      "episode: 9500 episode reward: 43.0 eps: 0.3828171404002276 avg reward (last 100): 31.00990099009901 episode loss:  247.17145\n",
      "episode: 9600 episode reward: 16.0 eps: 0.3790078566929674 avg reward (last 100): 29.10891089108911 episode loss:  226.46367\n",
      "episode: 9700 episode reward: 21.0 eps: 0.37523647787770664 avg reward (last 100): 25.722772277227723 episode loss:  420.49893\n",
      "episode: 9800 episode reward: 14.0 eps: 0.37150262677570317 avg reward (last 100): 23.980198019801982 episode loss:  433.1814\n",
      "episode: 9900 episode reward: 13.0 eps: 0.36780592996139233 avg reward (last 100): 28.940594059405942 episode loss:  409.50095\n",
      "episode: 10000 episode reward: 15.0 eps: 0.3641460177250418 avg reward (last 100): 31.15841584158416 episode loss:  359.62802\n",
      "episode: 10100 episode reward: 31.0 eps: 0.3605225240357743 avg reward (last 100): 25.08910891089109 episode loss:  302.2872\n",
      "episode: 10200 episode reward: 98.0 eps: 0.35693508650496253 avg reward (last 100): 27.752475247524753 episode loss:  420.47055\n",
      "episode: 10300 episode reward: 63.0 eps: 0.35338334634998586 avg reward (last 100): 28.85148514851485 episode loss:  411.20508\n",
      "episode: 10400 episode reward: 16.0 eps: 0.3498669483583473 avg reward (last 100): 29.326732673267326 episode loss:  645.37354\n",
      "episode: 10500 episode reward: 24.0 eps: 0.34638554085215006 avg reward (last 100): 31.940594059405942 episode loss:  407.128\n",
      "episode: 10600 episode reward: 15.0 eps: 0.34293877565292435 avg reward (last 100): 29.653465346534652 episode loss:  151.26959\n",
      "episode: 10700 episode reward: 11.0 eps: 0.3395263080468064 avg reward (last 100): 32.257425742574256 episode loss:  530.5374\n",
      "episode: 10800 episode reward: 14.0 eps: 0.33614779675006357 avg reward (last 100): 29.138613861386137 episode loss:  233.72699\n",
      "episode: 10900 episode reward: 45.0 eps: 0.3328029038749622 avg reward (last 100): 30.92079207920792 episode loss:  429.98145\n",
      "episode: 11000 episode reward: 10.0 eps: 0.3294912948959744 avg reward (last 100): 40.12871287128713 episode loss:  231.3626\n",
      "episode: 11100 episode reward: 80.0 eps: 0.3262126386163234 avg reward (last 100): 36.16831683168317 episode loss:  809.9944\n",
      "episode: 11200 episode reward: 14.0 eps: 0.32296660713485886 avg reward (last 100): 33.79207920792079 episode loss:  541.4645\n",
      "episode: 11300 episode reward: 20.0 eps: 0.3197528758132631 avg reward (last 100): 38.0 episode loss:  417.79315\n",
      "episode: 11400 episode reward: 11.0 eps: 0.31657112324358533 avg reward (last 100): 33.08910891089109 episode loss:  331.13324\n",
      "episode: 11500 episode reward: 70.0 eps: 0.3134210312160961 avg reward (last 100): 32.633663366336634 episode loss:  254.64044\n",
      "episode: 11600 episode reward: 27.0 eps: 0.3103022846874634 avg reward (last 100): 42.118811881188115 episode loss:  256.38324\n",
      "episode: 11700 episode reward: 12.0 eps: 0.3072145717492451 avg reward (last 100): 37.9009900990099 episode loss:  314.56863\n",
      "episode: 11800 episode reward: 11.0 eps: 0.30415758359669304 avg reward (last 100): 32.56435643564357 episode loss:  705.2514\n",
      "episode: 11900 episode reward: 9.0 eps: 0.30113101449787155 avg reward (last 100): 31.653465346534652 episode loss:  649.4339\n",
      "episode: 12000 episode reward: 14.0 eps: 0.2981345617630795 avg reward (last 100): 31.287128712871286 episode loss:  252.66443\n",
      "episode: 12100 episode reward: 26.0 eps: 0.2951679257145789 avg reward (last 100): 31.10891089108911 episode loss:  240.74106\n",
      "episode: 12200 episode reward: 44.0 eps: 0.29223080965662274 avg reward (last 100): 29.663366336633665 episode loss:  287.0636\n",
      "episode: 12300 episode reward: 12.0 eps: 0.28932291984578346 avg reward (last 100): 34.26732673267327 episode loss:  231.90034\n",
      "episode: 12400 episode reward: 16.0 eps: 0.2864439654615747 avg reward (last 100): 25.84158415841584 episode loss:  185.98524\n",
      "episode: 12500 episode reward: 10.0 eps: 0.28359365857736635 avg reward (last 100): 29.861386138613863 episode loss:  150.55313\n",
      "episode: 12600 episode reward: 31.0 eps: 0.2807717141315883 avg reward (last 100): 27.504950495049506 episode loss:  480.0328\n",
      "episode: 12700 episode reward: 22.0 eps: 0.2779778498992219 avg reward (last 100): 26.564356435643564 episode loss:  640.4972\n",
      "episode: 12800 episode reward: 64.0 eps: 0.27521178646357397 avg reward (last 100): 31.96039603960396 episode loss:  451.8057\n",
      "episode: 12900 episode reward: 13.0 eps: 0.2724732471883324 avg reward (last 100): 39.67326732673267 episode loss:  466.18777\n",
      "episode: 13000 episode reward: 17.0 eps: 0.2697619581898993 avg reward (last 100): 35.742574257425744 episode loss:  230.37784\n",
      "episode: 13100 episode reward: 25.0 eps: 0.26707764830999914 avg reward (last 100): 33.148514851485146 episode loss:  535.72736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13200 episode reward: 45.0 eps: 0.26442004908856137 avg reward (last 100): 34.73267326732673 episode loss:  483.48557\n",
      "episode: 13300 episode reward: 27.0 eps: 0.2617888947368702 avg reward (last 100): 36.118811881188115 episode loss:  543.8843\n",
      "episode: 13400 episode reward: 31.0 eps: 0.25918392211098357 avg reward (last 100): 33.42574257425743 episode loss:  772.10986\n",
      "episode: 13500 episode reward: 11.0 eps: 0.2566048706854152 avg reward (last 100): 38.366336633663366 episode loss:  196.72615\n",
      "episode: 13600 episode reward: 110.0 eps: 0.25405148252708043 avg reward (last 100): 32.13861386138614 episode loss:  384.07675\n",
      "episode: 13700 episode reward: 40.0 eps: 0.25152350226949866 avg reward (last 100): 38.475247524752476 episode loss:  501.1154\n",
      "episode: 13800 episode reward: 68.0 eps: 0.24902067708725495 avg reward (last 100): 34.693069306930695 episode loss:  637.8317\n",
      "episode: 13900 episode reward: 45.0 eps: 0.24654275667071443 avg reward (last 100): 33.13861386138614 episode loss:  517.9154\n",
      "episode: 14000 episode reward: 18.0 eps: 0.24408949320098863 avg reward (last 100): 42.13861386138614 episode loss:  891.36536\n",
      "episode: 14100 episode reward: 8.0 eps: 0.24166064132515097 avg reward (last 100): 34.18811881188119 episode loss:  197.35541\n",
      "episode: 14200 episode reward: 19.0 eps: 0.23925595813169845 avg reward (last 100): 40.28712871287129 episode loss:  322.05978\n",
      "episode: 14300 episode reward: 38.0 eps: 0.23687520312625857 avg reward (last 100): 35.16831683168317 episode loss:  406.14185\n",
      "episode: 14400 episode reward: 95.0 eps: 0.23451813820753678 avg reward (last 100): 39.504950495049506 episode loss:  358.71844\n",
      "episode: 14500 episode reward: 40.0 eps: 0.23218452764350356 avg reward (last 100): 42.79207920792079 episode loss:  358.8266\n",
      "episode: 14600 episode reward: 14.0 eps: 0.22987413804781923 avg reward (last 100): 40.148514851485146 episode loss:  524.669\n",
      "episode: 14700 episode reward: 20.0 eps: 0.22758673835649254 avg reward (last 100): 35.198019801980195 episode loss:  1032.843\n",
      "episode: 14800 episode reward: 15.0 eps: 0.22532209980477175 avg reward (last 100): 43.118811881188115 episode loss:  219.39482\n",
      "episode: 14900 episode reward: 62.0 eps: 0.22307999590426558 avg reward (last 100): 30.81188118811881 episode loss:  743.3319\n",
      "episode: 15000 episode reward: 27.0 eps: 0.22086020242029208 avg reward (last 100): 43.28712871287129 episode loss:  510.0066\n",
      "episode: 15100 episode reward: 84.0 eps: 0.21866249734945267 avg reward (last 100): 37.42574257425743 episode loss:  676.128\n",
      "episode: 15200 episode reward: 31.0 eps: 0.21648666089742943 avg reward (last 100): 44.35643564356435 episode loss:  388.18118\n",
      "episode: 15300 episode reward: 26.0 eps: 0.2143324754570033 avg reward (last 100): 49.06930693069307 episode loss:  955.1634\n",
      "episode: 15400 episode reward: 92.0 eps: 0.21219972558629083 avg reward (last 100): 40.881188118811885 episode loss:  677.5834\n",
      "episode: 15500 episode reward: 11.0 eps: 0.21008819798719794 avg reward (last 100): 44.92079207920792 episode loss:  766.18524\n",
      "episode: 15600 episode reward: 49.0 eps: 0.20799768148408734 avg reward (last 100): 42.89108910891089 episode loss:  598.69696\n",
      "episode: 15700 episode reward: 11.0 eps: 0.20592796700265928 avg reward (last 100): 42.960396039603964 episode loss:  876.62695\n",
      "episode: 15800 episode reward: 23.0 eps: 0.20387884754904154 avg reward (last 100): 46.31683168316832 episode loss:  269.8095\n",
      "episode: 15900 episode reward: 23.0 eps: 0.20185011818908757 avg reward (last 100): 45.12871287128713 episode loss:  185.03421\n",
      "episode: 16000 episode reward: 31.0 eps: 0.1998415760278814 avg reward (last 100): 38.97029702970297 episode loss:  385.8367\n",
      "episode: 16100 episode reward: 12.0 eps: 0.1978530201894455 avg reward (last 100): 43.93069306930693 episode loss:  893.54614\n",
      "episode: 16200 episode reward: 94.0 eps: 0.1958842517966513 avg reward (last 100): 47.43564356435643 episode loss:  552.73895\n",
      "episode: 16300 episode reward: 125.0 eps: 0.19393507395132883 avg reward (last 100): 50.43564356435643 episode loss:  411.32043\n",
      "episode: 16400 episode reward: 186.0 eps: 0.19200529171457545 avg reward (last 100): 49.118811881188115 episode loss:  417.97998\n",
      "episode: 16500 episode reward: 34.0 eps: 0.1900947120872594 avg reward (last 100): 52.75247524752475 episode loss:  168.44615\n",
      "episode: 16600 episode reward: 91.0 eps: 0.1882031439907179 avg reward (last 100): 45.68316831683168 episode loss:  759.9149\n",
      "episode: 16700 episode reward: 64.0 eps: 0.18633039824764733 avg reward (last 100): 59.7029702970297 episode loss:  482.43015\n",
      "episode: 16800 episode reward: 83.0 eps: 0.18447628756318316 avg reward (last 100): 46.53465346534654 episode loss:  473.16003\n",
      "episode: 16900 episode reward: 30.0 eps: 0.1826406265061689 avg reward (last 100): 51.64356435643565 episode loss:  925.8123\n",
      "episode: 17000 episode reward: 17.0 eps: 0.1808232314906104 avg reward (last 100): 56.16831683168317 episode loss:  188.40675\n",
      "episode: 17100 episode reward: 12.0 eps: 0.17902392075731569 avg reward (last 100): 50.48514851485149 episode loss:  290.31073\n",
      "episode: 17200 episode reward: 12.0 eps: 0.17724251435571686 avg reward (last 100): 50.0990099009901 episode loss:  423.06174\n",
      "episode: 17300 episode reward: 122.0 eps: 0.1754788341258734 avg reward (last 100): 40.73267326732673 episode loss:  223.23871\n",
      "episode: 17400 episode reward: 84.0 eps: 0.17373270368065385 avg reward (last 100): 43.82178217821782 episode loss:  549.05585\n",
      "episode: 17500 episode reward: 200.0 eps: 0.17200394838809538 avg reward (last 100): 42.0990099009901 episode loss:  377.0692\n",
      "episode: 17600 episode reward: 23.0 eps: 0.17029239535393864 avg reward (last 100): 40.54455445544554 episode loss:  327.7776\n",
      "episode: 17700 episode reward: 30.0 eps: 0.16859787340433657 avg reward (last 100): 42.475247524752476 episode loss:  379.59018\n",
      "episode: 17800 episode reward: 40.0 eps: 0.166920213068735 avg reward (last 100): 42.68316831683168 episode loss:  507.7191\n",
      "episode: 17900 episode reward: 72.0 eps: 0.16525924656292404 avg reward (last 100): 43.445544554455445 episode loss:  690.8244\n",
      "episode: 18000 episode reward: 52.0 eps: 0.16361480777225723 avg reward (last 100): 42.475247524752476 episode loss:  902.45544\n",
      "episode: 18100 episode reward: 18.0 eps: 0.16198673223503932 avg reward (last 100): 38.75247524752475 episode loss:  116.98701\n",
      "episode: 18200 episode reward: 40.0 eps: 0.16037485712607702 avg reward (last 100): 41.198019801980195 episode loss:  401.56015\n",
      "episode: 18300 episode reward: 38.0 eps: 0.15877902124039578 avg reward (last 100): 43.21782178217822 episode loss:  491.57822\n",
      "episode: 18400 episode reward: 15.0 eps: 0.15719906497711716 avg reward (last 100): 45.28712871287129 episode loss:  380.79117\n",
      "episode: 18500 episode reward: 72.0 eps: 0.15563483032349676 avg reward (last 100): 55.87128712871287 episode loss:  230.29085\n",
      "episode: 18600 episode reward: 16.0 eps: 0.1540861608391218 avg reward (last 100): 55.53465346534654 episode loss:  554.18585\n",
      "episode: 18700 episode reward: 14.0 eps: 0.15255290164026486 avg reward (last 100): 55.40594059405941 episode loss:  289.61374\n",
      "episode: 18800 episode reward: 11.0 eps: 0.1510348993843941 avg reward (last 100): 49.277227722772274 episode loss:  468.36926\n",
      "episode: 18900 episode reward: 27.0 eps: 0.14953200225483734 avg reward (last 100): 49.76237623762376 episode loss:  286.49475\n",
      "episode: 19000 episode reward: 39.0 eps: 0.14804405994559847 avg reward (last 100): 43.98019801980198 episode loss:  566.36194\n",
      "episode: 19100 episode reward: 56.0 eps: 0.14657092364632562 avg reward (last 100): 39.75247524752475 episode loss:  661.47253\n",
      "episode: 19200 episode reward: 22.0 eps: 0.14511244602742823 avg reward (last 100): 44.772277227722775 episode loss:  454.77698\n",
      "episode: 19300 episode reward: 28.0 eps: 0.14366848122534273 avg reward (last 100): 47.772277227722775 episode loss:  327.5963\n",
      "episode: 19400 episode reward: 53.0 eps: 0.14223888482794425 avg reward (last 100): 44.613861386138616 episode loss:  315.92575\n",
      "episode: 19500 episode reward: 27.0 eps: 0.1408235138601044 avg reward (last 100): 46.62376237623762 episode loss:  1076.403\n",
      "episode: 19600 episode reward: 34.0 eps: 0.1394222267693917 avg reward (last 100): 52.198019801980195 episode loss:  229.29097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 19700 episode reward: 90.0 eps: 0.13803488341191486 avg reward (last 100): 46.16831683168317 episode loss:  731.533\n",
      "episode: 19800 episode reward: 55.0 eps: 0.13666134503830696 avg reward (last 100): 49.37623762376238 episode loss:  248.58652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for i in hidden_units:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "                i, activation='tanh', kernel_initializer='RandomNormal'))\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            num_actions, activation='linear', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = MyModel(num_states, hidden_units, num_actions)\n",
    "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
    "\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "            return 0\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
    "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.predict(np.atleast_2d(states))[0])\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())\n",
    "\n",
    "\n",
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            reward = -200\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss = TrainNet.train(TargetNet)\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, mean(losses)\n",
    "\n",
    "def make_video(env, TrainNet):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = TrainNet.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "    num_states = len(env.observation_space.sample())\n",
    "    num_actions = env.action_space.n\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    N = 50000\n",
    "    total_rewards = np.empty(N)\n",
    "    epsilon = 0.99\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    for n in range(N):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward, losses = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "            tf.summary.scalar('average loss)', losses, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "                  \"episode loss: \", losses)\n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "    make_video(env, TrainNet)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(3):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
